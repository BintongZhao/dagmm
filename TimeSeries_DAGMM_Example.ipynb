{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series DAGMM for Anomaly Detection\n",
    "\n",
    "This notebook demonstrates the use of Deep Autoencoding Gaussian Mixture Model (DAGMM) adapted for time series anomaly detection.\n",
    "\n",
    "## Key Features:\n",
    "- LSTM-based encoder-decoder for temporal modeling\n",
    "- Handles variable-length sequences with masking\n",
    "- Time series specific reconstruction features\n",
    "- GMM-based anomaly scoring\n",
    "- Comprehensive evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Time Series DAGMM Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_timeseries import TimeSeriesDaGMM\n",
    "from data_loader_timeseries import (\n",
    "    create_synthetic_dataset, \n",
    "    get_timeseries_loader,\n",
    "    TimeSeriesDataset,\n",
    "    SyntheticTimeSeriesGenerator\n",
    ")\n",
    "from solver_timeseries import TimeSeriesSolver\n",
    "from utils_timeseries import (\n",
    "    plot_sample_sequences,\n",
    "    plot_reconstruction_comparison,\n",
    "    plot_anomaly_scores,\n",
    "    evaluate_anomaly_detection\n",
    ")\n",
    "\n",
    "print(\"Time Series DAGMM components imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Synthetic Time Series Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "N_NORMAL = 800\n",
    "N_ANOMALOUS = 200\n",
    "SEQ_LENGTH_RANGE = (50, 150)\n",
    "N_FEATURES = 10  # Using smaller feature dimension for demo\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "print(\"Creating synthetic time series dataset...\")\n",
    "\n",
    "# Create dataset\n",
    "train_dataset, test_dataset = create_synthetic_dataset(\n",
    "    n_normal=N_NORMAL,\n",
    "    n_anomalous=N_ANOMALOUS,\n",
    "    seq_length_range=SEQ_LENGTH_RANGE,\n",
    "    n_features=N_FEATURES,\n",
    "    test_split=TEST_SPLIT,\n",
    "    normalization='standard'\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Check label distribution\n",
    "train_labels = [train_dataset[i][1].item() for i in range(len(train_dataset))]\n",
    "test_labels = [test_dataset[i][1].item() for i in range(len(test_dataset))]\n",
    "\n",
    "print(f\"\\nTraining set - Normal: {train_labels.count(0)}, Anomalous: {train_labels.count(1)}\")\n",
    "print(f\"Test set - Normal: {test_labels.count(0)}, Anomalous: {test_labels.count(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Sample Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample sequences\n",
    "plot_sample_sequences(train_dataset, n_samples=4)\n",
    "plt.suptitle('Sample Time Series from Training Set', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Show sequence length distribution\n",
    "train_lengths = [train_dataset[i][2] for i in range(len(train_dataset))]\n",
    "test_lengths = [test_dataset[i][2] for i in range(len(test_dataset))]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_lengths, bins=20, alpha=0.7, label='Training')\n",
    "plt.hist(test_lengths, bins=20, alpha=0.7, label='Test')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sequence Length Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "normal_lengths = [train_dataset[i][2] for i in range(len(train_dataset)) if train_dataset[i][1] == 0]\n",
    "anomaly_lengths = [train_dataset[i][2] for i in range(len(train_dataset)) if train_dataset[i][1] == 1]\n",
    "plt.hist(normal_lengths, bins=15, alpha=0.7, label='Normal')\n",
    "plt.hist(anomaly_lengths, bins=15, alpha=0.7, label='Anomalous')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sequence Length by Class (Training)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = get_timeseries_loader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = get_timeseries_loader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test a batch\n",
    "for batch_idx, (sequences, labels, lengths, masks) in enumerate(train_loader):\n",
    "    print(f\"\\nFirst batch:\")\n",
    "    print(f\"Sequences shape: {sequences.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Lengths shape: {lengths.shape}\")\n",
    "    print(f\"Masks shape: {masks.shape}\")\n",
    "    print(f\"Sample lengths: {lengths[:5].tolist()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Time Series DAGMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "model_config = {\n",
    "    'input_dim': N_FEATURES,\n",
    "    'latent_dim': 16,\n",
    "    'hidden_dim': 32,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.2,\n",
    "    'bidirectional': True,\n",
    "    'n_gmm': 3\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = TimeSeriesDaGMM(**model_config)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model created and moved to {device}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass with a small batch\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get a small batch\n",
    "    for sequences, labels, lengths, masks in train_loader:\n",
    "        sequences = sequences[:4].to(device)  # Take only 4 samples\n",
    "        masks = masks[:4].to(device)\n",
    "        \n",
    "        print(f\"Input shape: {sequences.shape}\")\n",
    "        print(f\"Mask shape: {masks.shape}\")\n",
    "        \n",
    "        # Forward pass\n",
    "        latent, decoded, z, gamma = model(sequences, masks)\n",
    "        \n",
    "        print(f\"\\nForward pass results:\")\n",
    "        print(f\"Latent shape: {latent.shape}\")\n",
    "        print(f\"Decoded shape: {decoded.shape}\")\n",
    "        print(f\"Feature vector z shape: {z.shape}\")\n",
    "        print(f\"GMM probabilities gamma shape: {gamma.shape}\")\n",
    "        \n",
    "        # Test loss computation\n",
    "        loss, energy, recon_error, cov_diag = model.loss_function(\n",
    "            sequences, decoded, z, gamma, masks\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nLoss components:\")\n",
    "        print(f\"Total loss: {loss.item():.4f}\")\n",
    "        print(f\"Energy: {energy.item():.4f}\")\n",
    "        print(f\"Reconstruction error: {recon_error.item():.4f}\")\n",
    "        print(f\"Covariance diagonal: {cov_diag.item():.4f}\")\n",
    "        \n",
    "        break\n",
    "\n",
    "print(\"\\nForward pass test completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_config = {\n",
    "    'lr': 1e-3,\n",
    "    'num_epochs': 20,  # Reduced for demo\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'input_dim': N_FEATURES,\n",
    "    'latent_dim': model_config['latent_dim'],\n",
    "    'hidden_dim': model_config['hidden_dim'],\n",
    "    'num_layers': model_config['num_layers'],\n",
    "    'dropout': model_config['dropout'],\n",
    "    'bidirectional': model_config['bidirectional'],\n",
    "    'n_gmm': model_config['n_gmm'],\n",
    "    'lambda_energy': 0.1,\n",
    "    'lambda_cov_diag': 0.005,\n",
    "    'log_step': 5,\n",
    "    'model_save_step': 50,\n",
    "    'use_cuda': torch.cuda.is_available(),\n",
    "    'model_save_path': './demo_models',\n",
    "    'log_path': './demo_logs'\n",
    "}\n",
    "\n",
    "# Create solver\n",
    "solver = TimeSeriesSolver(train_loader, test_loader, training_config)\n",
    "\n",
    "print(\"Training setup completed!\")\n",
    "print(f\"Configuration: {training_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "\n",
    "train_losses = solver.train()\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, 'b-', linewidth=2)\n",
    "plt.title('Training Loss Curve', fontsize=16)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {train_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation and Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "print(\"Starting model evaluation...\")\n",
    "\n",
    "test_metrics = solver.test(threshold_percentile=95)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in test_metrics.items():\n",
    "    if not np.isnan(value):\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "# Additional detailed evaluation\n",
    "solver.model.eval()\n",
    "all_energies = []\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "threshold = test_metrics['threshold']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, labels, lengths, masks in test_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        latent, decoded, z, gamma = solver.model(sequences, masks)\n",
    "        sample_energy, _ = solver.model.compute_energy(z, size_average=False)\n",
    "        \n",
    "        energies = sample_energy.cpu().numpy()\n",
    "        predictions = (energies > threshold).astype(int)\n",
    "        \n",
    "        all_energies.extend(energies)\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "all_energies = np.array(all_energies)\n",
    "all_labels = np.array(all_labels)\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=['Normal', 'Anomalous']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot anomaly scores\n",
    "plot_anomaly_scores(all_energies, all_labels, threshold)\n",
    "plt.suptitle('Anomaly Detection Results', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Anomalous'],\n",
    "            yticklabels=['Normal', 'Anomalous'])\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Error analysis\n",
    "false_positives = (all_labels == 0) & (all_predictions == 1)\n",
    "false_negatives = (all_labels == 1) & (all_predictions == 0)\n",
    "true_positives = (all_labels == 1) & (all_predictions == 1)\n",
    "true_negatives = (all_labels == 0) & (all_predictions == 0)\n",
    "\n",
    "print(f\"\\nError Analysis:\")\n",
    "print(f\"True Negatives: {np.sum(true_negatives)}\")\n",
    "print(f\"False Positives: {np.sum(false_positives)}\")\n",
    "print(f\"False Negatives: {np.sum(false_negatives)}\")\n",
    "print(f\"True Positives: {np.sum(true_positives)}\")\n",
    "\n",
    "if np.sum(false_positives) > 0:\n",
    "    print(f\"\\nFalse Positive energies (mean): {all_energies[false_positives].mean():.4f}\")\n",
    "if np.sum(false_negatives) > 0:\n",
    "    print(f\"False Negative energies (mean): {all_energies[false_negatives].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Reconstruction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions for normal and anomalous samples\n",
    "solver.model.eval()\n",
    "\n",
    "# Find some normal and anomalous samples\n",
    "normal_indices = np.where(all_labels == 0)[0][:2]\n",
    "anomaly_indices = np.where(all_labels == 1)[0][:2]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "subplot_idx = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_count = 0\n",
    "    for sequences, labels, lengths, masks in test_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        latent, decoded, z, gamma = solver.model(sequences, masks)\n",
    "        \n",
    "        # Move back to CPU for plotting\n",
    "        sequences_cpu = sequences.cpu()\n",
    "        decoded_cpu = decoded.cpu()\n",
    "        masks_cpu = masks.cpu()\n",
    "        \n",
    "        for i in range(sequences.shape[0]):\n",
    "            current_idx = sample_count + i\n",
    "            \n",
    "            if current_idx in normal_indices or current_idx in anomaly_indices:\n",
    "                label = \"Normal\" if current_idx in normal_indices else \"Anomalous\"\n",
    "                \n",
    "                # Get valid length\n",
    "                valid_len = masks_cpu[i].sum().item()\n",
    "                orig_seq = sequences_cpu[i, :valid_len, :3]  # First 3 features\n",
    "                recon_seq = decoded_cpu[i, :valid_len, :3]\n",
    "                \n",
    "                # Original\n",
    "                plt.subplot(4, 2, subplot_idx)\n",
    "                for j in range(3):\n",
    "                    plt.plot(orig_seq[:, j], alpha=0.8, label=f'Feature {j+1}')\n",
    "                plt.title(f'{label} - Original (Energy: {all_energies[current_idx]:.3f})')\n",
    "                plt.ylabel('Value')\n",
    "                if subplot_idx == 1:\n",
    "                    plt.legend()\n",
    "                \n",
    "                # Reconstructed\n",
    "                plt.subplot(4, 2, subplot_idx + 1)\n",
    "                for j in range(3):\n",
    "                    plt.plot(recon_seq[:, j], alpha=0.8, label=f'Feature {j+1}')\n",
    "                plt.title(f'{label} - Reconstructed')\n",
    "                plt.ylabel('Value')\n",
    "                if subplot_idx == 1:\n",
    "                    plt.legend()\n",
    "                \n",
    "                subplot_idx += 2\n",
    "                \n",
    "                if subplot_idx > 8:\n",
    "                    break\n",
    "        \n",
    "        sample_count += sequences.shape[0]\n",
    "        \n",
    "        if subplot_idx > 8:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Original vs Reconstructed Sequences', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the learned features\n",
    "solver.model.eval()\n",
    "\n",
    "all_z_features = []\n",
    "all_latents = []\n",
    "feature_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, labels, lengths, masks in test_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        latent, decoded, z, gamma = solver.model(sequences, masks)\n",
    "        \n",
    "        all_z_features.append(z.cpu().numpy())\n",
    "        all_latents.append(latent.cpu().numpy())\n",
    "        feature_labels.extend(labels.numpy())\n",
    "\n",
    "all_z_features = np.vstack(all_z_features)\n",
    "all_latents = np.vstack(all_latents)\n",
    "feature_labels = np.array(feature_labels)\n",
    "\n",
    "print(f\"Feature analysis:\")\n",
    "print(f\"Latent features shape: {all_latents.shape}\")\n",
    "print(f\"Combined features shape: {all_z_features.shape}\")\n",
    "\n",
    "# Plot feature distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Latent features (first few dimensions)\n",
    "for i in range(min(3, all_latents.shape[1])):\n",
    "    normal_feat = all_latents[feature_labels == 0, i]\n",
    "    anomaly_feat = all_latents[feature_labels == 1, i]\n",
    "    \n",
    "    axes[i].hist(normal_feat, bins=30, alpha=0.7, label='Normal', density=True)\n",
    "    axes[i].hist(anomaly_feat, bins=30, alpha=0.7, label='Anomalous', density=True)\n",
    "    axes[i].set_title(f'Latent Feature {i+1}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Temporal features (last few dimensions of z)\n",
    "temporal_features = ['MSE', 'MAE', 'DTW-like', 'Trend']\n",
    "for i, name in enumerate(temporal_features[:3]):\n",
    "    feat_idx = all_z_features.shape[1] - 4 + i\n",
    "    normal_feat = all_z_features[feature_labels == 0, feat_idx]\n",
    "    anomaly_feat = all_z_features[feature_labels == 1, feat_idx]\n",
    "    \n",
    "    axes[i+3].hist(normal_feat, bins=30, alpha=0.7, label='Normal', density=True)\n",
    "    axes[i+3].hist(anomaly_feat, bins=30, alpha=0.7, label='Anomalous', density=True)\n",
    "    axes[i+3].set_title(f'Temporal Feature: {name}')\n",
    "    axes[i+3].legend()\n",
    "    axes[i+3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Distributions: Normal vs Anomalous', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Model Interpretation and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model parameters\n",
    "print(\"Model Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# GMM parameters\n",
    "phi = solver.model.phi.cpu().numpy()\n",
    "mu = solver.model.mu.cpu().numpy()\n",
    "cov = solver.model.cov.cpu().numpy()\n",
    "\n",
    "print(f\"GMM Component Weights (phi): {phi}\")\n",
    "print(f\"Component weights sum: {phi.sum():.4f}\")\n",
    "\n",
    "print(f\"\\nGMM Component Means shape: {mu.shape}\")\n",
    "for i, component_mean in enumerate(mu):\n",
    "    print(f\"Component {i+1} mean magnitude: {np.linalg.norm(component_mean):.4f}\")\n",
    "\n",
    "print(f\"\\nGMM Covariance matrices shape: {cov.shape}\")\n",
    "for i, component_cov in enumerate(cov):\n",
    "    eigenvals = np.linalg.eigvals(component_cov)\n",
    "    print(f\"Component {i+1} eigenvalue range: [{eigenvals.min():.4f}, {eigenvals.max():.4f}]\")\n",
    "\n",
    "# Feature importance analysis\n",
    "print(f\"\\nFeature Importance Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Compute feature correlations with anomaly scores\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "feature_names = [f'Latent_{i}' for i in range(all_latents.shape[1])] + \\\n",
    "                ['MSE', 'MAE', 'DTW_like', 'Trend']\n",
    "\n",
    "correlations = []\n",
    "for i in range(all_z_features.shape[1]):\n",
    "    corr, p_val = pearsonr(all_z_features[:, i], all_energies)\n",
    "    correlations.append((feature_names[i], corr, p_val))\n",
    "\n",
    "# Sort by absolute correlation\n",
    "correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"Top features correlated with anomaly scores:\")\n",
    "for name, corr, p_val in correlations[:8]:\n",
    "    significance = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
    "    print(f\"{name:15}: {corr:6.3f} {significance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time Series DAGMM Demo Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "print(f\"  - Test samples: {len(test_dataset)}\")\n",
    "print(f\"  - Features: {N_FEATURES}\")\n",
    "print(f\"  - Sequence length range: {SEQ_LENGTH_RANGE}\")\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  - LSTM Encoder: {model_config['bidirectional']} bidirectional, {model_config['num_layers']} layers\")\n",
    "print(f\"  - Hidden dimension: {model_config['hidden_dim']}\")\n",
    "print(f\"  - Latent dimension: {model_config['latent_dim']}\")\n",
    "print(f\"  - GMM components: {model_config['n_gmm']}\")\n",
    "print(f\"  - Total parameters: {total_params:,}\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  - Epochs: {training_config['num_epochs']}\")\n",
    "print(f\"  - Learning rate: {training_config['lr']}\")\n",
    "print(f\"  - Final loss: {train_losses[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "for key, value in test_metrics.items():\n",
    "    if not np.isnan(value):\n",
    "        print(f\"  - {key.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nKey Features:\")\n",
    "print(f\"  ✓ Handles variable-length sequences with masking\")\n",
    "print(f\"  ✓ LSTM-based temporal modeling\")\n",
    "print(f\"  ✓ Time series specific reconstruction features\")\n",
    "print(f\"  ✓ GMM-based anomaly scoring\")\n",
    "print(f\"  ✓ End-to-end trainable architecture\")\n",
    "\n",
    "print(f\"\\nThis implementation successfully adapts DAGMM for time series anomaly detection,\")\n",
    "print(f\"achieving {test_metrics['accuracy']:.1%} accuracy on the synthetic dataset.\")\n",
    "print(f\"\\nThe model can be easily extended for real-world time series applications!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}